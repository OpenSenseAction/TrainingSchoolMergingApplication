{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import traceback as tb\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#DEBUG_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HBV_setup import daa_optimize as HBVd_calib\n",
    "from HBV_setup import dab_validate as HBVd_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intoduction: The HBV Model and Ahr catchment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "We would like to thank Faizen Anwer from TU Munich for providing the HBV model used in this training school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display catchment\n",
    "The cell below loads and plots a map of the two catchments and the sensor data. Frist the one for the official rain gauges from the DWD and the federal state of Rhineland-Palatinate. Both data sets are used for the operational interplation product (Intermet) which were used as input for the interpolation of the catchment precipitation time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load location of sensors:\n",
    "df_pws = pd.read_csv(\"metadata/pws_coords.csv\",sep=';')\n",
    "df_dwd = pd.read_csv(\"metadata/dwd_coords.csv\",sep=',')\n",
    "df_rlp = pd.read_csv(\"metadata/coords_rlp.csv\",sep=';')\n",
    "df_altenahr_shape = pd.read_csv(\"metadata/Altenahr_shape.csv\")\n",
    "df_kreuzberg_shape = pd.read_csv(\"metadata/Kreuzberg_shape.csv\")\n",
    "\n",
    "# todo: -add the location of the stream gauges\n",
    "#       -add rivers?\n",
    "plt.plot(df_kreuzberg_shape.lon,df_kreuzberg_shape.lat,color='grey',label=\"catchment Kreuzberg\")\n",
    "plt.plot(df_altenahr_shape.lon,df_altenahr_shape.lat,color='black',label=\"catchment Altenahr\")\n",
    "plt.scatter(df_rlp.lon,df_rlp.lat,color='C2',label=\"RLP rain gauge\", alpha=0.8)\n",
    "plt.scatter(df_dwd.lon,df_dwd.lat,color='C0',label='DWD rain gauge',alpha=0.8)\n",
    "plt.xlim(6.5,7.2)\n",
    "plt.ylim(50.2,50.6)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('deg [°]')\n",
    "plt.ylabel('deg [°]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- How to you judge the coverage of rain gauges in these two catchments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HBV modeling with daily data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load daily HBV input data for calibration period from 2001 to 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration procedure involves following steps\n",
    "- read calibration data\n",
    "- set ranges for model parameters\n",
    "- set catchment area \n",
    "- run optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'daily/inputs/daily_2001_2011/daily_input_data_10420.csv'\n",
    "daily_data_calib_10420 = pd.read_csv(\n",
    "    input_path, \n",
    "    sep=';', \n",
    "    index_col=0, \n",
    "    parse_dates=True)\n",
    "daily_data_calib_10420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function for plotting discharge and precipiation data. It takes a dataframe with preipitation and Discharge data as inputand you can specify the time spane by using `beg_time`and `end_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_pcp(df, beg_time='2001-01-01', end_time='2021-01-01'):\n",
    "    time = df[beg_time:end_time].index\n",
    "    q = df.dis_ref[beg_time:end_time].values\n",
    "    p = df[beg_time:end_time].ppt.values\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    plt.clf()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.plot(time, q, color=[.5, .5, .5])\n",
    "    ax.set_ylim([0, max(q) * 2])\n",
    "    ax.set_ylabel('Q [m³/s]')\n",
    "    ax.set_xlabel('time')\n",
    "\n",
    "   # Rotate x-axis labels diagonally\n",
    "    plt.xticks(rotation=45, ha='right')  # ha='right' aligns labels properly\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    #ax2.bar(time, p, width=1, color=[0, .14, .5], edgecolor=[.7, .7, .7], alpha=.9, linewidth=.1)\n",
    "    ax2.plot(time, p, color=[0, .14, .5], linestyle='-', linewidth=1.5, label=\"Precipitation (Pcp)\")\n",
    "    ax2.set_ylim([max(p) * 2, 0])\n",
    "    ax2.set_ylabel('Pcp [mm / time_step]')\n",
    "    \n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify the time span of displayed precipitation and runoff data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_q_pcp(daily_data_calib_10420, beg_time='2001-01-01', end_time='2021-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameter bounds\n",
    "Set upper and lower bounds for the model calibration (paramerter optimization). To save a computational time, we have fixed some of the parameter values in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prms_buds_dict = {\n",
    "    # snow storage parameters\n",
    "    'snw_dth': (33.00, 33.00),  # Initial depth [L]\n",
    "    'snw_ast': (-0.62, -0.62),  # Air snow TEM [K].\n",
    "    'snw_amt': (-0.54, -0.54),  # Air melt TEM [K].\n",
    "    'snw_amf': (1.9, 1.9),      # Air melt factor [L/TK].\n",
    "    'snw_pmf': (0.65, 0.65),    # PPT melt factor [L/LTK].\n",
    "    #soil storage parameters\n",
    "    'sl0_mse': (5.66, 5.66),    # Soil 0 initial depth [L].\n",
    "    'sl1_mse': (197.00, 197),   # Soil 1 initial depth [L].\n",
    "    'sl0_fcy': (0.00, 2e+2),    # Field capacity [L].\n",
    "    'sl0_bt0': (0.00, 3.00),    # Beta [-]. \n",
    "    'sl1_pwp': (0.00, 4e+2),    # PWP [L].\n",
    "    'sl1_fcy': (0.00, 4e+2),    # Field capacity [L].\n",
    "    'sl1_bt0': (2.5, 2.50),     # Beta [-].\n",
    "    # uppper reservoir parameters\n",
    "    'urr_dth': (6.1, 6.1),      # URR initial depth [L].\n",
    "    'urr_rsr': (0.00, 1.00),    # Runoff split ratio [-].\n",
    "    'urr_tdh': (0.00, 1e+2),    # Threshold depth [L].\n",
    "    'urr_tdr': (0.00, 1.00),    # Threshold DIS const. [1/T].\n",
    "    'urr_cst': (0.00, 1.00),    # RNF const. [1/T].\n",
    "    'urr_dro': (0.00, 1.00),    # DIS ratio [-].\n",
    "    'urr_ulc': (0.00, 1.00),    # URR-to-LRR const. [1/T].\n",
    "    # lower reservoir data\n",
    "    'lrr_dth': (1.14, 1.14),    # LRR initial depth [L].\n",
    "    'lrr_tdh': (0.00, 1e+4),    # Threshold depth [L]\n",
    "    'lrr_cst': (0.00, 1.00),    # Runoff const. [1/T].\n",
    "    'lrr_dro': (0.00, 1.00),    # Discharge ratio [-].\n",
    "    }\n",
    "\n",
    "prms_long_names = {\n",
    "    # snow storage parameters\n",
    "    'snw_dth': \"Initial depth (L)\", 'snw_ast': \"Air snow TEM (K)\",\n",
    "    'snw_amt': \"Air melt TEM (K)\", 'snw_amf': \"Air melt factor (L/TK)\",\n",
    "    'snw_pmf': \"PPT melt factor (L/LTK)\",\n",
    "    #soil storage parameters\n",
    "    'sl0_mse': \"Soil 0 initial depth (L)\", 'sl1_mse': \"Soil 1 initial depth (L)\",\n",
    "    'sl0_fcy': \"Field capacity (L)\",  'sl0_bt0': \"Beta (-)\",\n",
    "    'sl1_pwp': \"PWP (L)\",  'sl1_fcy': \"Field capacity (L)\",\n",
    "    'sl1_bt0': \"Beta (-)\",\n",
    "    # uppper reservoir parameters\n",
    "    'urr_dth': \"URR initial depth (L)\", 'urr_rsr': \"Runoff split ratio (-)\",\n",
    "    'urr_tdh': \"Threshold depth (L)\", 'urr_tdr': \"Threshold DIS const  (1/T)\",\n",
    "    'urr_cst': \"RNF const  (1/T)\", 'urr_dro': \"DIS ratio (-)\",\n",
    "    'urr_ulc': \"URR-to-LRR const  (1/T)\", \n",
    "    # lower reservoir data\n",
    "    'lrr_dth': \"LRR initial depth (L)\", 'lrr_tdh': \"Threshold depth (L)\",    # Threshold depth [L]\n",
    "    'lrr_cst': \"Runoff const  (1/T)\", 'lrr_dro': \"Discharge ratio (-)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model calibration\n",
    "Feed the calibration function with:\n",
    "- parameter dictionary\n",
    "- input DataFrame\n",
    "- catchment area\n",
    "- output directory, where optimized parameters and other optimization outputs will be stored as csv files\n",
    "- catchment label, string used in a name of the generated files\n",
    "\n",
    "The following code will start the model calibration. Depending on the the Performance of your computer this may take several minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_area_10420 = 749117129.0 # catchment area in [m^2]\n",
    "HBVd_calib.main(\n",
    "    prms_buds_dict, \n",
    "    daily_data_calib_10420, \n",
    "    cat_area = c_area_10420, \n",
    "    output_dir=r'daily/calibration_hbv_daily', \n",
    "    cat_label = '10420')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be stored in a folder called `HBV/daily/calib_results_hbv_daily` (relative path in your working directory set above).\n",
    "You will find here following files:\n",
    "- \"prms_{cat_label}_sr.csv\" - optimized parameter values\n",
    "- \"prf_{cat_label}_sr.csv\" - performance metrics\n",
    "- \"dis_sim_{cat_label}_df.csv\" - simulated and reference flows\n",
    "- \"sim_{cat_label}_otps_df.csv\" - simulated state variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance during calibration\n",
    "Let's read scores evaluating the model calibration and judge the model/calibration performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_prf_calib_10420 = pd.read_csv(\n",
    "    r'daily/calibration_hbv_daily/prf_10420_sr.csv', \n",
    "    sep=';', \n",
    "    skiprows = 1, \n",
    "    names = ['score', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_prf_calib_10420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "- How to you judge the Model performance based on Nash-Sutcliffe (NS) and above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's validate the model with data from 2011-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model parameters and input data\n",
    "\n",
    "Load the model parameters (optimized in the previous steps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prms_path = Path(rf'daily/calibration_hbv_daily/prms_10420_sr.csv')\n",
    "daily_prms_10420 = pd.read_csv(prms_path, sep=';', index_col=0).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input data for the validation period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'daily/inputs/daily_2011_2025/daily_input_data_10420.csv'\n",
    "daily_data_valid_10420 = pd.read_csv(input_path, sep=';', index_col=0, parse_dates=True)\n",
    "daily_data_valid_10420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBVd_valid.main(daily_prms_10420, daily_data_valid_10420, cat_area = c_area_10420, secs_per_step = 86400,\n",
    "                output_dir=r'daily/validation_hbv_daily', cat_label = '10420')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model performance\n",
    "load the performance scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load performance scores\n",
    "daily_prf_valid_10420 = pd.read_csv(\n",
    "    r'daily/validation_hbv_daily/prf_10420_sr.csv', \n",
    "    sep=';', \n",
    "    skiprows = 1, \n",
    "    names = ['score', 'value'])\n",
    "daily_prf_valid_10420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare scores for the calibration and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors and labels for each point\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'grey']\n",
    "labels = daily_prf_valid_10420.score\n",
    "\n",
    "# Create scatter plot\n",
    "for i in range(1,6):\n",
    "    plt.scatter(\n",
    "        daily_prf_valid_10420.value[i], \n",
    "        daily_prf_calib_10420.value[i], \n",
    "        color=colors[i], label=labels[i], \n",
    "        edgecolors='k', s=100)\n",
    "\n",
    "# Add identity line (y = x)\n",
    "plt.plot([0,5], [0,5], linestyle='--', color='black', label=\"Identity Line\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Daily PRF Valid Values\")\n",
    "plt.ylabel(\"Daily PRF Calib Values\")\n",
    "plt.title(\"Scores for validation and calibration - catchment 10420\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Set axis limits to match the identity line range\n",
    "plt.xlim(0.5, 1)\n",
    "plt.ylim(0.5, 1)\n",
    "\n",
    "# Show grid and plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "- How to you judge the Model performance based on Nash-Sutcliffe (NS) above for the validation period?\n",
    "- What could be the reasons that the NS worse?\n",
    "\n",
    "### Compare simulated and reference flow time series\n",
    "Plot the observed and simulatied discharges for the validation period 2016-2022 using the function below. You can adjust the time span by specifying start and end dates in .main(), e.g. (beg_time='2021-01-01',end_time='2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sim_ref_ts (dis_df, beg_time='2021-07-01', end_time='2021-08-01'):\n",
    "    dis_df = dis_df.loc[beg_time:end_time,:]\n",
    "    \n",
    "    plt.plot(dis_df.index, dis_df['ref'].values, label='REF', alpha=0.8)\n",
    "    plt.plot(dis_df.index, dis_df['sim'].values, label='SIM', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Time [day]')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel('Discharge [m$^3$.s$^{-1}$]')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads simulated and reference data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sim_valid_10420 = pd.read_csv(\n",
    "    r'daily/validation_hbv_daily/dis_sim_10420_df.csv', \n",
    "    sep=';', \n",
    "    index_col = 0)\n",
    "daily_sim_valid_10420.index = pd.to_datetime(\n",
    "    daily_sim_valid_10420.index, \n",
    "    format='%Y-%m-%d')\n",
    "daily_sim_valid_10420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1:\n",
    "Plot observed and simulated discharge in the variable `daily_sim_valid_10420` using the `plot_sim_ref_ts`function and explore differences during peak flows. You can also look at yearly flow maxima using e.g. this chunk of code to identify them: `peak_flow_dates = daily_sim_valid_10420.resample('YE')['ref'].apply(lambda x: x.idxmax())`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter code here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input (\"Enter 'Solution' to display solution: \") == 'Solution':\n",
    "    %load solutions/1a_solution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input (\"Enter 'Solution' to display solution: \") == 'Solution':\n",
    "    %load solutions/1b_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "\n",
    "-The peak discharge during the flood event in July 2021 at the gauge Altenahr was ~1200 m³/s. What is the issue here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Have a look at the hourly discharges between 14 and 16 July 2021 and plot these. The code below loads the hourly input data into a data frame, the observed discharge is named `hourly_ref`\n",
    "\n",
    "Plot hourly and daily reference flows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "hourly_data_valid_10420=pd.read_csv(r'hourly/inputs/Intermet_gauge/hbv_input_data_10420.csv.zip',\n",
    "                                    sep=';', index_col=0, parse_dates=True)\n",
    "\n",
    "# Plot hourly data (blue line) and daily data (red dashed line with markers)\n",
    "hourly_data_valid_10420.dis_ref['2021-07-12':'2021-07-17 00:00'].plot(\n",
    "    label=\"Hourly Data\", linestyle='-', color='blue', alpha=0.7)\n",
    "daily_data_valid_10420.dis_ref['2021-07-12':'2021-07-17'].plot(\n",
    "    label=\"Daily Data\", linestyle='--', color='red', marker='o', alpha=0.8)\n",
    "\n",
    "# Labels, titles \n",
    "plt.xlabel('Time [hh:mm, date]')\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
    "plt.ylabel('Discharge [m$^3$.s$^{-1}$]')\n",
    "plt.title(\"Discharge Data: Daily vs. Hourly Resolution\")\n",
    "\n",
    "# Add legend with solid white background\n",
    "plt.legend(facecolor='white', edgecolor='black')  # Edge color makes it visible\n",
    "plt.grid(True) # Show grid\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare hydrograph volumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_volume = 3600 * sum(hourly_data_valid_10420.dis_ref['2021-07-12':'2021-07-17 00:00'].values[:1] + \n",
    "                                  hourly_data_valid_10420.dis_ref['2021-07-12':'2021-07-17 00:00'].values[:-1]) / 2\n",
    "daily_volume = 3600 * 24 * sum(daily_data_valid_10420.dis_ref['2021-07-12':'2021-07-17'].values[:1] + \n",
    "                                      daily_data_valid_10420.dis_ref['2021-07-12':'2021-07-17'].values[:-1]) / 2\n",
    "print(f\"Hourly runoff volume in Mio m³: {round(1e-6 * hourly_volume, 2)}\")\n",
    "print(f\"Dialy runoff volume in Mio m³: {round(1e-6 * daily_volume, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. HBV modeling with hourly data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen above, a daily model resolution is not suffcient to capture the flood peaks in smaller, fast reacting catchments. Before we investigate the influences of different (OS) rainfall inputs, we will look how the model works with standard (Intermet gauge) rainfall data at hourly resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model validation with hourly data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the hourly data for the Altenahr catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'hourly/inputs/Intermet_gauge/hbv_input_data_10420.csv.zip'\n",
    "hourly_data_valid_10420 = pd.read_csv(input_path, sep=';', index_col=0, parse_dates=True)\n",
    "hourly_data_valid_10420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal reference flow rain\n",
    "hourly_data_valid_10420.dis_ref.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Use the `plot_q_pcp`function for plotting discharge and precipiation data. Plotting all hourly data might take some time, so consider specifying a time by using `beg_time`and `end_time`, e.g. for 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Enter 'Solution' to display solutions: \")=='Solution':\n",
    "    %load solutions/2_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- What do you observe with respect to precipitation and discharge from May - August 2021?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Run model with hourly data\n",
    "Run model with hourly data. Use model parameters obtained in the previous exercise (calibration with daily data).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBVd_valid.main(daily_prms_10420, inp_dfe = hourly_data_valid_10420, cat_area = c_area_10420,\n",
    "                secs_per_step = 3600, output_dir=r'hourly/validation_Intermet', cat_label = '10420')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_sim_valid_10420 = pd.read_csv(r'hourly/validation_Intermet/dis_sim_10420_df.csv', sep=';', index_col = 0)\n",
    "hourly_sim_valid_10420.index = pd.to_datetime(hourly_sim_valid_10420.index, format='%Y-%m-%d %H:%M:%S')\n",
    "#dis_df\n",
    "plot_sim_ref_ts(hourly_sim_valid_10420, beg_time='2021-07-11', end_time='2021-07-21')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What ist the issue here? Why is the peak now overestimated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with hourly data once again\n",
    "We did calibration with hourly input data for you, so just load the optimized parameters and compare them with daily-data based parameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prms_path = Path(rf'hourly/calibration_Intermet_gauge/prms_10420_sr.csv')\n",
    "hourly_prms_10420 = pd.read_csv(prms_path, sep=';', index_col=0).iloc[:, 0]\n",
    "\n",
    "df = pd.concat([pd.Series(prms_long_names), daily_prms_10420, hourly_prms_10420], axis=1)\n",
    "df.columns = ['Long names', 'Daily calibration', 'Hourly calibration']  # Rename columns if needed\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Run model again with parameters optimized on hourly Intermet interploated gauge data. They are stored now in `hourly_prms_10420`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Enter 'Solution' to display solutions: \")=='Solution':\n",
    "    %load solutions/3_solution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_prf_valid_10420_new = pd.read_csv(\n",
    "    r'hourly/validation_Intermet/prf_10420_sr.csv', \n",
    "    sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_prf_valid_10420_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot simulation with hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store old simluation into a separate variable\n",
    "hourly_sim_valid_10420_new = pd.read_csv(\n",
    "    r'hourly/validation_Intermet/dis_sim_10420_df.csv', \n",
    "    sep=';', \n",
    "    index_col = 0)\n",
    "hourly_sim_valid_10420_new.index = pd.to_datetime(\n",
    "    hourly_sim_valid_10420_new.index, \n",
    "    format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_old = hourly_sim_valid_10420['2021-07-11':'2021-07-21']\n",
    "ts_new = hourly_sim_valid_10420_new['2021-07-11':'2021-07-21']\n",
    "\n",
    "plt.plot(ts_old.index, ts_old['ref'].values, label = 'REF', alpha = 0.8, lw = 3)\n",
    "plt.plot(ts_old.index, ts_old['sim'].values, label = 'SIM_old', alpha = 0.8, color = 'grey', ls = '--')\n",
    "plt.plot(ts_new.index, ts_new['sim'].values, label = 'SIM_new', alpha = 0.8, color = 'darkorange', lw = 2)\n",
    "\n",
    "\n",
    "plt.xlabel('Time [day]')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel('Discharge [m$^3$.s$^{-1}$]')\n",
    "\n",
    "plt.grid()\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Rainfall from OS Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot a map showing the location of PWS(and CMLs?) together with the DWD and RLP rain guages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_kreuzberg_shape.lon,df_kreuzberg_shape.lat,color='grey',label=\"catchment Kreuzberg\")\n",
    "plt.plot(df_altenahr_shape.lon,df_altenahr_shape.lat,color='black',label=\"catchment Altenahr\")\n",
    "plt.scatter(df_rlp.lon,df_rlp.lat,color='C2',label=\"RLP rain gauge\", alpha=0.8)\n",
    "plt.scatter(df_pws.lon,df_pws.lat,color='C1',label=\"PWS rain gauge\",alpha=0.8)\n",
    "plt.scatter(df_dwd.lon,df_dwd.lat,color='C0',label='DWD rain gauge',alpha=0.8)\n",
    "plt.xlim(6.5,7.2)\n",
    "plt.ylim(50.2,50.6)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"solutions/os_map.png\" alt=\"Description\" style=\"width:600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the different precipitation datasets (official and OS) for the event itself which was on 14/15 July 2021. First for the Altenahr catchment. The following precipitation datasets are available for the two catachments {ID}:\n",
    "- `Intermet`: Interpolated DWD and RLP gauge data\n",
    "- `RW_cml`: CML adjusted radar data\n",
    "- `RW_gauge`: gauge adjusted radar data\n",
    "- `RW_gauge_cml`: CML and gauge adjusted radar data\n",
    "- `dwd`: Interpolation from DWD rain gauges\n",
    "- `dwd_pws`: Interpolation from DWD rain gauges and PWS\n",
    "- `dwd_pws_cml`: Interpolation from DWD rain gauges, PWS and CML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: load and plot the different data sets for the two catchments. The data are located in the folder `/data/OS_pcp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Altenahr (ID: 10420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_pcp_10420=pd.read_csv(r'hourly/OS_pcp/os_pcp_10420.csv', sep=';', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_pcp_10420.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kreuzberg (ID: 10460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_pcp_10460=pd.read_csv(r'hourly/OS_pcp/os_pcp_10460.csv', sep=';', index_col=0, parse_dates=True)\n",
    "os_pcp_10460.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question: What do you observe here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Running the model with OS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the model with different rainfall inputs. Tempreture and PET is always the same. THe files with modified precipitation data are in the folders Inputs and subfoders \n",
    "- `Intermet_gauge`: Interpolated DWD and RLP gauge data\n",
    "- `RW_cml`: CML adjusted radar data\n",
    "- `RW_gauge`: gauge adjusted radar data\n",
    "- `RW_gauge_cml`: CML and gauge adjusted radar data\n",
    "- `dwd`: Interpolation from DWD rain gauges\n",
    "- `dwd_pws`: Interpolation from DWD rain gauges and PWS\n",
    "- `dwd_pws_cml`: Interpolation from DWD rain gauges, PWS and CML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "- Modify string in dataset variable to match the folder names with different rainfall inputs (above) and run the validation.\n",
    "in the cells below, compare the time series.\n",
    "- Run the simulation for Kreuzberg catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'dwd_pws'\n",
    "input_dir_name = r'hourly/inputs/' + dataset\n",
    "output_dir_name = r'hourly/validation_' + dataset\n",
    "\n",
    "ID='10420'\n",
    "hourly_valid_10420 = pd.read_csv(input_dir_name + '/hbv_input_data_' + ID + '.csv.zip', sep=';',\n",
    "                                          index_col=0, parse_dates=True)\n",
    "c_area_10420 = 749117129.0 # area of the catchment in m2\n",
    "HBVd_valid.main(hourly_prms_10420, hourly_valid_10420, cat_area = c_area_10420, secs_per_step = 3600,\n",
    "                output_dir = output_dir_name, cat_label = ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hourly_sim_valid_10420 = pd.read_csv(output_dir_name + '/dis_sim_10420_df.csv', sep=';', index_col = 0)\n",
    "hourly_sim_valid_10420.index = pd.to_datetime(hourly_sim_valid_10420.index, format='%Y-%m-%d %H:%M:%S')\n",
    "#dis_df\n",
    "plot_sim_ref_ts(hourly_sim_valid_10420, beg_time='2021-07-13', end_time='2021-07-17')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kreuzberg catchment simulation\n",
    "Use the different OS precipitation data for the Kreuzberg Catchment as done above by changing the `dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_area_10460 = 45117129.0 # catchment area in [m^2]\n",
    "prms_path = Path(rf'hourly/calibration_Intermet_gauge/prms_10460_sr.csv')\n",
    "hourly_prms_10460 = pd.read_csv(prms_path, sep=';', index_col=0).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Intermet_gauge'\n",
    "input_dir_name = r'hourly/inputs/' + dataset\n",
    "output_dir_name = r'hourly/validation_' + dataset\n",
    "\n",
    "ID = '10460'\n",
    "hourly_valid_10460 = pd.read_csv(input_dir_name + '/hbv_input_data_' + ID + '.csv.zip',\n",
    "                                 sep=';', index_col=0, parse_dates=True)\n",
    "c_area_10460 = 45*1e6 # area of the catchment in m2\n",
    "hourly_valid_10460['dis_ref'] = hourly_valid_10460['dis_ref'] + 0.001\n",
    "\n",
    "HBVd_valid.main(hourly_prms_10460, hourly_valid_10460, cat_area = c_area_10460, secs_per_step = 3600,\n",
    "                output_dir = output_dir_name, cat_label = ID)\n",
    "\n",
    "\n",
    "#min(hourly_RW_gauge_valid_10460['dis_ref'])\n",
    "#HBVd_valid.main(main_dir=path, ID=ID, dir_name=dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hourly_sim_valid_10460 = pd.read_csv(output_dir_name + '/dis_sim_10460_df.csv', sep=';', index_col = 0)\n",
    "hourly_sim_valid_10460.index = pd.to_datetime(hourly_sim_valid_10460.index, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#dis_df\n",
    "plot_sim_ref_ts(hourly_sim_valid_10460, beg_time='2021-07-13', end_time='2021-07-17')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "- What do you observe when you use different datasets for the model?\n",
    "- Why are results not always optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Spatial patterns of different OS precipitation data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise we can look at and comapre the spatial rainfall data from different precipitation datasets. You will find all the data sets as netCDF files in the folder `hourly/OS_PCP`. These datasets contain data from xxx to xxx and are as follows:\n",
    "- `dwd.nc` : Interpolated rainfall from DWD and RLP rain gauges\n",
    "- `dwd_pws.nc` : Interpolated rainfall from DWD,RLP and QC'ed PWS\n",
    "- `dwd_pws_cml.nc` : Interpolated rainfall from DWD,RLP and QC'ed PWS and CML\n",
    "- `RW_gauge.nc` : Gauge adjusted radar data\n",
    "- `RW_cml.nc` : CML adjusted radar data\n",
    "- `RW_gauge_cml.nc` : Gauge and CML adjusted radar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example how you can load and explore these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load RW_cml data set\n",
    "ds_RW_cml=xr.open_dataset(r'hourly/OS_pcp/RW_cml.nc')\n",
    "ds_dwd_pws=xr.open_dataset(r'hourly/OS_pcp/dwd_pws.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_RW_cml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot maps and explore the data and spatial patterns. The variable in the RW datasets is called `RW`, in the dwd datsets `rainfall`. You have several options:\n",
    " - plot a time step, e.g. (`isel(time=30)`) \n",
    " - plot the sum for the whole time by using `sum(dim='time')`\n",
    " - by adding `plot(x='longitudes', y='latitudes')`you can plot the map with Lat/Lon coordinates\n",
    " - you can also plot difference maps by substracating two datasats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a map with the data for the first time step (isel(time=30)). The variable is called 'RW'\n",
    "ds_RW_cml.isel(time=30).RW.plot(x='longitudes', y='latitudes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_dwd_pws.rainfall[24:41].sum(dim='time')-ds_RW_cml.RW[24:41].sum(dim='time')).plot(x='longitudes', y='latitudes')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
